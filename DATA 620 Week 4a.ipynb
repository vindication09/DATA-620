{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment – High Frequency Words\n",
    "\n",
    "Group 4\n",
    "- Santosh Cheruku\n",
    "- Vinicio Haro\n",
    "- Javern Wilson\n",
    "- Saayed Alam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement \n",
    "\n",
    "1. Choose a corpus of interest.\n",
    "2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting,\n",
    "defensible way).\n",
    "3. Taking the most common words, how many unique words represent half of the total words in the corpus?\n",
    "4. Identify the 200 highest frequency words in this corpus.\n",
    "5. Create a graph that shows the relative frequency of these 200 words.\n",
    "6. Does the observed relative frequency of these words follow Zipf’s law? Explain.\n",
    "7. In what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download required packages\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a Corpus of Interest\n",
    "\n",
    "Project Guttenberg is a site that allows you to read books directly from the HTML or as a standard txt file on the browser. Because of this, it is an ideal to use as a corpus because we do not need to download data locally. We can pull data directly from the URL thus making this work reproducible. \n",
    "\n",
    "For our corpus, we will be using the book BEALBY by H. G. Wells.\n",
    "\n",
    "http://www.gutenberg.org/files/59769/59769-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/59769/59769-0.txt\"\n",
    "\n",
    "response = request.urlopen(url)\n",
    "\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "\n",
    "The raw text file contains the parts of the book that we do not want to use in our analysis. We do not want to use the publication page nor the chapters page. Thankfully we can impliment methodology to choose where and when to start reading the text into our notebook. \n",
    "\n",
    "We will also use the word_tokenize function to convert our raw text into a tokenized version.\n",
    "https://kite.com/python/docs/nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83914"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_position = raw.find(\"CHAPTER I\")\n",
    "\n",
    "ending_position = raw.find(\"End of the Project Gutenberg EBook of Bealby; A Holiday, by H. G. Wells\")\n",
    "\n",
    "raw = raw[starting_position:ending_position]\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "words = [w for w in tokens if w.isalpha()]\n",
    "\n",
    "words = [w.lower() for w in words]\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have converted the text into tokens, we want to look at the the number of words. In our case we have 83,914 tokenized words. Converting to tokens will make text operations much more efficient. It should be a data prep step when preparing raw data for some downstream analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting, defensible way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
